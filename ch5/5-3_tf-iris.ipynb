{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "iris_data = pd.read_csv(\"iris.csv\", encoding=\"utf-8\")\n",
    "\n",
    "y_labels = iris_data.loc[:, \"Name\"]\n",
    "x_data = iris_data.loc[:, [\"SepalLength\", \"SepalWidth\", \"PetalLength\", \"PetalWidth\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Iris-setosa\n",
       "1    Iris-setosa\n",
       "2    Iris-setosa\n",
       "3    Iris-setosa\n",
       "4    Iris-setosa\n",
       "Name: Name, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SepalLength</th>\n",
       "      <th>SepalWidth</th>\n",
       "      <th>PetalLength</th>\n",
       "      <th>PetalWidth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SepalLength  SepalWidth  PetalLength  PetalWidth\n",
       "0          5.1         3.5          1.4         0.2\n",
       "1          4.9         3.0          1.4         0.2\n",
       "2          4.7         3.2          1.3         0.2\n",
       "3          4.6         3.1          1.5         0.2\n",
       "4          5.0         3.6          1.4         0.2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = {\n",
    "    'Iris-setosa': [1, 0, 0],\n",
    "    'Iris-versicolor': [0, 1, 0],\n",
    "    'Iris-virginica': [0, 0, 1]\n",
    "}\n",
    "y_nums = list(map(lambda v:labels[v], y_labels))\n",
    "y_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_nums, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 4])\n",
    "y_ = tf.placeholder(tf.float32, [None, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.Variable(tf.zeros([4, 3]))\n",
    "b = tf.Variable(tf.zeros([3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.nn.softmax(tf.matmul(x, w) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = -tf.reduce_sum(y_ * tf.log(y))\n",
    "optimizer = tf.train.AdamOptimizer(0.05)\n",
    "train = optimizer.minimize(cross_entropy)\n",
    "\n",
    "predict = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(predict, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "train_feed_dict = {x: x_train, y_: y_train}\n",
    "for step in range(300):\n",
    "    sess.run(train, feed_dict=train_feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.96666664\n"
     ]
    }
   ],
   "source": [
    "acc = sess.run(accuracy, feed_dict={x: x_test, y_: y_test})\n",
    "print(\"Accuracy: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.contrib.keras as keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_nums_np = np.array(y_nums)\n",
    "x_data_np = np.array(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_data_np, y_nums_np, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\0190711653\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "Dense = keras.layers.Dense\n",
    "model = keras.models.Sequential()\n",
    "model.add(Dense(10, activation='relu', input_shape=(4,)))\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 120 samples\n",
      "Epoch 1/300\n",
      "120/120 [==============================] - 0s 259us/sample - loss: 1.3725 - acc: 0.3583\n",
      "Epoch 2/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 1.2135 - acc: 0.3583\n",
      "Epoch 3/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 1.0655 - acc: 0.3667\n",
      "Epoch 4/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.9614 - acc: 0.4250\n",
      "Epoch 5/300\n",
      "120/120 [==============================] - 0s 50us/sample - loss: 0.8881 - acc: 0.5750\n",
      "Epoch 6/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.8374 - acc: 0.6417\n",
      "Epoch 7/300\n",
      "120/120 [==============================] - 0s 50us/sample - loss: 0.7997 - acc: 0.6750\n",
      "Epoch 8/300\n",
      "120/120 [==============================] - 0s 50us/sample - loss: 0.7704 - acc: 0.6917\n",
      "Epoch 9/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.7449 - acc: 0.7083\n",
      "Epoch 10/300\n",
      "120/120 [==============================] - 0s 59us/sample - loss: 0.7237 - acc: 0.7500\n",
      "Epoch 11/300\n",
      "120/120 [==============================] - 0s 50us/sample - loss: 0.7041 - acc: 0.7333\n",
      "Epoch 12/300\n",
      "120/120 [==============================] - 0s 50us/sample - loss: 0.6864 - acc: 0.7417\n",
      "Epoch 13/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.6699 - acc: 0.7583\n",
      "Epoch 14/300\n",
      "120/120 [==============================] - 0s 50us/sample - loss: 0.6549 - acc: 0.7583\n",
      "Epoch 15/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.6409 - acc: 0.7667\n",
      "Epoch 16/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.6278 - acc: 0.7667\n",
      "Epoch 17/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.6158 - acc: 0.7667\n",
      "Epoch 18/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.6049 - acc: 0.7833\n",
      "Epoch 19/300\n",
      "120/120 [==============================] - 0s 50us/sample - loss: 0.5951 - acc: 0.7667\n",
      "Epoch 20/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.5855 - acc: 0.7583\n",
      "Epoch 21/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.5770 - acc: 0.7667\n",
      "Epoch 22/300\n",
      "120/120 [==============================] - 0s 50us/sample - loss: 0.5690 - acc: 0.7667\n",
      "Epoch 23/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.5610 - acc: 0.7750\n",
      "Epoch 24/300\n",
      "120/120 [==============================] - 0s 50us/sample - loss: 0.5533 - acc: 0.7750\n",
      "Epoch 25/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.5470 - acc: 0.7667\n",
      "Epoch 26/300\n",
      "120/120 [==============================] - 0s 58us/sample - loss: 0.5399 - acc: 0.7667\n",
      "Epoch 27/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.5336 - acc: 0.7750\n",
      "Epoch 28/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.5277 - acc: 0.7750\n",
      "Epoch 29/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.5218 - acc: 0.7833\n",
      "Epoch 30/300\n",
      "120/120 [==============================] - 0s 50us/sample - loss: 0.5163 - acc: 0.7833\n",
      "Epoch 31/300\n",
      "120/120 [==============================] - 0s 50us/sample - loss: 0.5108 - acc: 0.7833\n",
      "Epoch 32/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.5058 - acc: 0.7833\n",
      "Epoch 33/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.5006 - acc: 0.7833\n",
      "Epoch 34/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.4962 - acc: 0.7833\n",
      "Epoch 35/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.4912 - acc: 0.7833\n",
      "Epoch 36/300\n",
      "120/120 [==============================] - 0s 50us/sample - loss: 0.4871 - acc: 0.7833\n",
      "Epoch 37/300\n",
      "120/120 [==============================] - 0s 50us/sample - loss: 0.4834 - acc: 0.7917\n",
      "Epoch 38/300\n",
      "120/120 [==============================] - 0s 58us/sample - loss: 0.4780 - acc: 0.8000\n",
      "Epoch 39/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.4738 - acc: 0.8000\n",
      "Epoch 40/300\n",
      "120/120 [==============================] - 0s 50us/sample - loss: 0.4698 - acc: 0.8000\n",
      "Epoch 41/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.4666 - acc: 0.8000\n",
      "Epoch 42/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.4624 - acc: 0.8167\n",
      "Epoch 43/300\n",
      "120/120 [==============================] - 0s 50us/sample - loss: 0.4582 - acc: 0.8333\n",
      "Epoch 44/300\n",
      "120/120 [==============================] - 0s 50us/sample - loss: 0.4543 - acc: 0.8167\n",
      "Epoch 45/300\n",
      "120/120 [==============================] - 0s 58us/sample - loss: 0.4512 - acc: 0.8167\n",
      "Epoch 46/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.4473 - acc: 0.8167\n",
      "Epoch 47/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.4440 - acc: 0.8250\n",
      "Epoch 48/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.4406 - acc: 0.8417\n",
      "Epoch 49/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.4371 - acc: 0.8417\n",
      "Epoch 50/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.4337 - acc: 0.8583\n",
      "Epoch 51/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.4306 - acc: 0.8583\n",
      "Epoch 52/300\n",
      "120/120 [==============================] - 0s 58us/sample - loss: 0.4275 - acc: 0.8667\n",
      "Epoch 53/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.4243 - acc: 0.8667\n",
      "Epoch 54/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.4215 - acc: 0.8667\n",
      "Epoch 55/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.4183 - acc: 0.8583\n",
      "Epoch 56/300\n",
      "120/120 [==============================] - 0s 50us/sample - loss: 0.4156 - acc: 0.8917\n",
      "Epoch 57/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.4126 - acc: 0.9000\n",
      "Epoch 58/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.4094 - acc: 0.9250\n",
      "Epoch 59/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.4066 - acc: 0.9333\n",
      "Epoch 60/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.4037 - acc: 0.9333\n",
      "Epoch 61/300\n",
      "120/120 [==============================] - 0s 50us/sample - loss: 0.4011 - acc: 0.9250\n",
      "Epoch 62/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.3982 - acc: 0.9250\n",
      "Epoch 63/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.3958 - acc: 0.9333\n",
      "Epoch 64/300\n",
      "120/120 [==============================] - 0s 50us/sample - loss: 0.3934 - acc: 0.9333\n",
      "Epoch 65/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.3900 - acc: 0.9333\n",
      "Epoch 66/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.3875 - acc: 0.9417\n",
      "Epoch 67/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.3847 - acc: 0.9417\n",
      "Epoch 68/300\n",
      "120/120 [==============================] - 0s 50us/sample - loss: 0.3820 - acc: 0.9500\n",
      "Epoch 69/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.3797 - acc: 0.9500\n",
      "Epoch 70/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.3774 - acc: 0.9500\n",
      "Epoch 71/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.3747 - acc: 0.9583\n",
      "Epoch 72/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.3723 - acc: 0.9500\n",
      "Epoch 73/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.3695 - acc: 0.9583\n",
      "Epoch 74/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.3673 - acc: 0.9667\n",
      "Epoch 75/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.3645 - acc: 0.9667\n",
      "Epoch 76/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.3624 - acc: 0.9583\n",
      "Epoch 77/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.3597 - acc: 0.9500\n",
      "Epoch 78/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.3580 - acc: 0.9583\n",
      "Epoch 79/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.3551 - acc: 0.9667\n",
      "Epoch 80/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.3525 - acc: 0.9667\n",
      "Epoch 81/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.3498 - acc: 0.9667\n",
      "Epoch 82/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 42us/sample - loss: 0.3511 - acc: 0.9667\n",
      "Epoch 83/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.3452 - acc: 0.9667\n",
      "Epoch 84/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.3429 - acc: 0.9667\n",
      "Epoch 85/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.3411 - acc: 0.9667\n",
      "Epoch 86/300\n",
      "120/120 [==============================] - 0s 50us/sample - loss: 0.3385 - acc: 0.9667\n",
      "Epoch 87/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.3367 - acc: 0.9667\n",
      "Epoch 88/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.3339 - acc: 0.9667\n",
      "Epoch 89/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.3324 - acc: 0.9667\n",
      "Epoch 90/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.3296 - acc: 0.9667\n",
      "Epoch 91/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.3273 - acc: 0.9667\n",
      "Epoch 92/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.3254 - acc: 0.9667\n",
      "Epoch 93/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.3236 - acc: 0.9667\n",
      "Epoch 94/300\n",
      "120/120 [==============================] - 0s 50us/sample - loss: 0.3212 - acc: 0.9667\n",
      "Epoch 95/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.3187 - acc: 0.9667\n",
      "Epoch 96/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.3170 - acc: 0.9667\n",
      "Epoch 97/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.3145 - acc: 0.9750\n",
      "Epoch 98/300\n",
      "120/120 [==============================] - 0s 25us/sample - loss: 0.3124 - acc: 0.9750\n",
      "Epoch 99/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.3102 - acc: 0.9750\n",
      "Epoch 100/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.3082 - acc: 0.9750\n",
      "Epoch 101/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.3062 - acc: 0.9750\n",
      "Epoch 102/300\n",
      "120/120 [==============================] - 0s 50us/sample - loss: 0.3041 - acc: 0.9750\n",
      "Epoch 103/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.3020 - acc: 0.9750\n",
      "Epoch 104/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.3001 - acc: 0.9750\n",
      "Epoch 105/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.2991 - acc: 0.9750\n",
      "Epoch 106/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.2964 - acc: 0.9750\n",
      "Epoch 107/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.2947 - acc: 0.9750\n",
      "Epoch 108/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.2922 - acc: 0.9750\n",
      "Epoch 109/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.2900 - acc: 0.9750\n",
      "Epoch 110/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.2881 - acc: 0.9750\n",
      "Epoch 111/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.2864 - acc: 0.9750\n",
      "Epoch 112/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.2846 - acc: 0.9750\n",
      "Epoch 113/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.2827 - acc: 0.9750\n",
      "Epoch 114/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.2808 - acc: 0.9750\n",
      "Epoch 115/300\n",
      "120/120 [==============================] - 0s 50us/sample - loss: 0.2791 - acc: 0.9750\n",
      "Epoch 116/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.2767 - acc: 0.9750\n",
      "Epoch 117/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.2748 - acc: 0.9750\n",
      "Epoch 118/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.2732 - acc: 0.9750\n",
      "Epoch 119/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.2713 - acc: 0.9750\n",
      "Epoch 120/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.2700 - acc: 0.9750\n",
      "Epoch 121/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.2676 - acc: 0.9750\n",
      "Epoch 122/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.2662 - acc: 0.9750\n",
      "Epoch 123/300\n",
      "120/120 [==============================] - 0s 50us/sample - loss: 0.2642 - acc: 0.9750\n",
      "Epoch 124/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.2628 - acc: 0.9750\n",
      "Epoch 125/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.2612 - acc: 0.9750\n",
      "Epoch 126/300\n",
      "120/120 [==============================] - 0s 50us/sample - loss: 0.2601 - acc: 0.9750\n",
      "Epoch 127/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.2581 - acc: 0.9750\n",
      "Epoch 128/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.2556 - acc: 0.9750\n",
      "Epoch 129/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.2538 - acc: 0.9750\n",
      "Epoch 130/300\n",
      "120/120 [==============================] - 0s 50us/sample - loss: 0.2522 - acc: 0.9750\n",
      "Epoch 131/300\n",
      "120/120 [==============================] - 0s 50us/sample - loss: 0.2506 - acc: 0.9750\n",
      "Epoch 132/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.2490 - acc: 0.9750\n",
      "Epoch 133/300\n",
      "120/120 [==============================] - 0s 50us/sample - loss: 0.2479 - acc: 0.9750\n",
      "Epoch 134/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.2457 - acc: 0.9750\n",
      "Epoch 135/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.2441 - acc: 0.9750\n",
      "Epoch 136/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.2428 - acc: 0.9750\n",
      "Epoch 137/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.2410 - acc: 0.9750\n",
      "Epoch 138/300\n",
      "120/120 [==============================] - 0s 50us/sample - loss: 0.2398 - acc: 0.9750\n",
      "Epoch 139/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.2378 - acc: 0.9750\n",
      "Epoch 140/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.2364 - acc: 0.9833\n",
      "Epoch 141/300\n",
      "120/120 [==============================] - 0s 50us/sample - loss: 0.2351 - acc: 0.9833\n",
      "Epoch 142/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.2332 - acc: 0.9833\n",
      "Epoch 143/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.2321 - acc: 0.9833\n",
      "Epoch 144/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.2310 - acc: 0.9833\n",
      "Epoch 145/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.2294 - acc: 0.9833\n",
      "Epoch 146/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.2279 - acc: 0.9750\n",
      "Epoch 147/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.2261 - acc: 0.9750\n",
      "Epoch 148/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.2248 - acc: 0.9833\n",
      "Epoch 149/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.2230 - acc: 0.9833\n",
      "Epoch 150/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.2217 - acc: 0.9833\n",
      "Epoch 151/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.2206 - acc: 0.9833\n",
      "Epoch 152/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.2191 - acc: 0.9833\n",
      "Epoch 153/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.2178 - acc: 0.9833\n",
      "Epoch 154/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.2162 - acc: 0.9833\n",
      "Epoch 155/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.2152 - acc: 0.9833\n",
      "Epoch 156/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.2135 - acc: 0.9833\n",
      "Epoch 157/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.2122 - acc: 0.9833\n",
      "Epoch 158/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.2108 - acc: 0.9833\n",
      "Epoch 159/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.2095 - acc: 0.9833\n",
      "Epoch 160/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.2084 - acc: 0.9833\n",
      "Epoch 161/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.2072 - acc: 0.9833\n",
      "Epoch 162/300\n",
      "120/120 [==============================] - 0s 58us/sample - loss: 0.2056 - acc: 0.9833\n",
      "Epoch 163/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 42us/sample - loss: 0.2046 - acc: 0.9833\n",
      "Epoch 164/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.2031 - acc: 0.9833\n",
      "Epoch 165/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.2021 - acc: 0.9833\n",
      "Epoch 166/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.2009 - acc: 0.9833\n",
      "Epoch 167/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1998 - acc: 0.9833\n",
      "Epoch 168/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.1989 - acc: 0.9833\n",
      "Epoch 169/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1971 - acc: 0.9833\n",
      "Epoch 170/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.1962 - acc: 0.9833\n",
      "Epoch 171/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.1950 - acc: 0.9833\n",
      "Epoch 172/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1937 - acc: 0.9833\n",
      "Epoch 173/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.1944 - acc: 0.9833\n",
      "Epoch 174/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1914 - acc: 0.9833\n",
      "Epoch 175/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.1902 - acc: 0.9833\n",
      "Epoch 176/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.1898 - acc: 0.9833\n",
      "Epoch 177/300\n",
      "120/120 [==============================] - 0s 50us/sample - loss: 0.1884 - acc: 0.9833\n",
      "Epoch 178/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.1870 - acc: 0.9833\n",
      "Epoch 179/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.1862 - acc: 0.9833\n",
      "Epoch 180/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1849 - acc: 0.9833\n",
      "Epoch 181/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1836 - acc: 0.9833\n",
      "Epoch 182/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1829 - acc: 0.9833\n",
      "Epoch 183/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1821 - acc: 0.9833\n",
      "Epoch 184/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.1807 - acc: 0.9833\n",
      "Epoch 185/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1796 - acc: 0.9833\n",
      "Epoch 186/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.1788 - acc: 0.9833\n",
      "Epoch 187/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1778 - acc: 0.9833\n",
      "Epoch 188/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1771 - acc: 0.9833\n",
      "Epoch 189/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.1756 - acc: 0.9833\n",
      "Epoch 190/300\n",
      "120/120 [==============================] - 0s 50us/sample - loss: 0.1746 - acc: 0.9833\n",
      "Epoch 191/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1736 - acc: 0.9833\n",
      "Epoch 192/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.1734 - acc: 0.9833\n",
      "Epoch 193/300\n",
      "120/120 [==============================] - 0s 50us/sample - loss: 0.1717 - acc: 0.9833\n",
      "Epoch 194/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.1712 - acc: 0.9833\n",
      "Epoch 195/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1702 - acc: 0.9833\n",
      "Epoch 196/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1690 - acc: 0.9833\n",
      "Epoch 197/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1686 - acc: 0.9833\n",
      "Epoch 198/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1672 - acc: 0.9833\n",
      "Epoch 199/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.1667 - acc: 0.9833\n",
      "Epoch 200/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1654 - acc: 0.9833\n",
      "Epoch 201/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1644 - acc: 0.9833\n",
      "Epoch 202/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1638 - acc: 0.9833\n",
      "Epoch 203/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1635 - acc: 0.9833\n",
      "Epoch 204/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1621 - acc: 0.9833\n",
      "Epoch 205/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1611 - acc: 0.9833\n",
      "Epoch 206/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1609 - acc: 0.9833\n",
      "Epoch 207/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.1596 - acc: 0.9833\n",
      "Epoch 208/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.1590 - acc: 0.9833\n",
      "Epoch 209/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1584 - acc: 0.9833\n",
      "Epoch 210/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.1573 - acc: 0.9833\n",
      "Epoch 211/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1564 - acc: 0.9833\n",
      "Epoch 212/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1555 - acc: 0.9833\n",
      "Epoch 213/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1555 - acc: 0.9833\n",
      "Epoch 214/300\n",
      "120/120 [==============================] - 0s 50us/sample - loss: 0.1542 - acc: 0.9833\n",
      "Epoch 215/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.1533 - acc: 0.9833\n",
      "Epoch 216/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1525 - acc: 0.9833\n",
      "Epoch 217/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1516 - acc: 0.9833\n",
      "Epoch 218/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1509 - acc: 0.9833\n",
      "Epoch 219/300\n",
      "120/120 [==============================] - 0s 50us/sample - loss: 0.1514 - acc: 0.9833\n",
      "Epoch 220/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.1496 - acc: 0.9833\n",
      "Epoch 221/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1486 - acc: 0.9833\n",
      "Epoch 222/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1480 - acc: 0.9833\n",
      "Epoch 223/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1478 - acc: 0.9833\n",
      "Epoch 224/300\n",
      "120/120 [==============================] - 0s 50us/sample - loss: 0.1468 - acc: 0.9833\n",
      "Epoch 225/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1459 - acc: 0.9833\n",
      "Epoch 226/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1453 - acc: 0.9833\n",
      "Epoch 227/300\n",
      "120/120 [==============================] - 0s 50us/sample - loss: 0.1446 - acc: 0.9833\n",
      "Epoch 228/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.1439 - acc: 0.9833\n",
      "Epoch 229/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1435 - acc: 0.9833\n",
      "Epoch 230/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1426 - acc: 0.9833\n",
      "Epoch 231/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.1422 - acc: 0.9833\n",
      "Epoch 232/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1415 - acc: 0.9833\n",
      "Epoch 233/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.1412 - acc: 0.9833\n",
      "Epoch 234/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1401 - acc: 0.9833\n",
      "Epoch 235/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1405 - acc: 0.9833\n",
      "Epoch 236/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1388 - acc: 0.9833\n",
      "Epoch 237/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1389 - acc: 0.9833\n",
      "Epoch 238/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1376 - acc: 0.9833\n",
      "Epoch 239/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.1369 - acc: 0.9833\n",
      "Epoch 240/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1364 - acc: 0.9833\n",
      "Epoch 241/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.1359 - acc: 0.9833\n",
      "Epoch 242/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.1353 - acc: 0.9833\n",
      "Epoch 243/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1349 - acc: 0.9833\n",
      "Epoch 244/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 33us/sample - loss: 0.1342 - acc: 0.9833\n",
      "Epoch 245/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.1341 - acc: 0.9833\n",
      "Epoch 246/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1334 - acc: 0.9833\n",
      "Epoch 247/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.1326 - acc: 0.9833\n",
      "Epoch 248/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.1317 - acc: 0.9833\n",
      "Epoch 249/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1312 - acc: 0.9833\n",
      "Epoch 250/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1311 - acc: 0.9833\n",
      "Epoch 251/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.1304 - acc: 0.9833\n",
      "Epoch 252/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.1296 - acc: 0.9833\n",
      "Epoch 253/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1290 - acc: 0.9833\n",
      "Epoch 254/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1285 - acc: 0.9833\n",
      "Epoch 255/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.1280 - acc: 0.9833\n",
      "Epoch 256/300\n",
      "120/120 [==============================] - 0s 50us/sample - loss: 0.1279 - acc: 0.9833\n",
      "Epoch 257/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1273 - acc: 0.9833\n",
      "Epoch 258/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1269 - acc: 0.9833\n",
      "Epoch 259/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.1259 - acc: 0.9833\n",
      "Epoch 260/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.1261 - acc: 0.9833\n",
      "Epoch 261/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1256 - acc: 0.9833\n",
      "Epoch 262/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1248 - acc: 0.9833\n",
      "Epoch 263/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.1241 - acc: 0.9833\n",
      "Epoch 264/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1238 - acc: 0.9833\n",
      "Epoch 265/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.1240 - acc: 0.9833\n",
      "Epoch 266/300\n",
      "120/120 [==============================] - 0s 50us/sample - loss: 0.1226 - acc: 0.9833\n",
      "Epoch 267/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1227 - acc: 0.9833\n",
      "Epoch 268/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.1219 - acc: 0.9833\n",
      "Epoch 269/300\n",
      "120/120 [==============================] - 0s 50us/sample - loss: 0.1227 - acc: 0.9833\n",
      "Epoch 270/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.1212 - acc: 0.9833\n",
      "Epoch 271/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1203 - acc: 0.9833\n",
      "Epoch 272/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1198 - acc: 0.9833\n",
      "Epoch 273/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.1197 - acc: 0.9833\n",
      "Epoch 274/300\n",
      "120/120 [==============================] - 0s 50us/sample - loss: 0.1191 - acc: 0.9833\n",
      "Epoch 275/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1187 - acc: 0.9833\n",
      "Epoch 276/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.1182 - acc: 0.9833\n",
      "Epoch 277/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1179 - acc: 0.9833\n",
      "Epoch 278/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.1175 - acc: 0.9833\n",
      "Epoch 279/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1168 - acc: 0.9833\n",
      "Epoch 280/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1168 - acc: 0.9833\n",
      "Epoch 281/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.1163 - acc: 0.9833\n",
      "Epoch 282/300\n",
      "120/120 [==============================] - 0s 50us/sample - loss: 0.1159 - acc: 0.9833\n",
      "Epoch 283/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1157 - acc: 0.9833\n",
      "Epoch 284/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1148 - acc: 0.9833\n",
      "Epoch 285/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1145 - acc: 0.9833\n",
      "Epoch 286/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.1142 - acc: 0.9833\n",
      "Epoch 287/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1136 - acc: 0.9833\n",
      "Epoch 288/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.1133 - acc: 0.9833\n",
      "Epoch 289/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1133 - acc: 0.9833\n",
      "Epoch 290/300\n",
      "120/120 [==============================] - 0s 50us/sample - loss: 0.1127 - acc: 0.9833\n",
      "Epoch 291/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1122 - acc: 0.9833\n",
      "Epoch 292/300\n",
      "120/120 [==============================] - 0s 50us/sample - loss: 0.1118 - acc: 0.9833\n",
      "Epoch 293/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.1114 - acc: 0.9833\n",
      "Epoch 294/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.1112 - acc: 0.9833\n",
      "Epoch 295/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1108 - acc: 0.9833\n",
      "Epoch 296/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.1101 - acc: 0.9833\n",
      "Epoch 297/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1099 - acc: 0.9833\n",
      "Epoch 298/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1106 - acc: 0.9833\n",
      "Epoch 299/300\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.1107 - acc: 0.9833\n",
      "Epoch 300/300\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.1094 - acc: 0.9833\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e096dc9fd0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=20, epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 0s 532us/sample - loss: 0.1861 - acc: 0.9667\n",
      "Accuracy:  0.96666664 Loss:  0.18611489236354828\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(\"Accuracy: \", score[1], \"Loss: \", score[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
